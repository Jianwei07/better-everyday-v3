services:
  fastapi-backend:
    build:
      context: ./api # <-- point to /api directory
      dockerfile: Dockerfile # (optional; default if named Dockerfile)
    container_name: eva-fastapi
    env_file:
      - .env.local
    volumes:
      - ./chroma_storage:/api/chroma_storage
      - ./data:/api/data
    ports:
      - "8000:8000"
    depends_on:
      - llm-server
    restart: unless-stopped

  llm-server:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: eva-llama-cpp
    command: ["-m", "/models/phi-2.Q4_K_M.gguf", "--port", "8001", "-c", "1024"]
    volumes:
      - ./models:/models
    ports:
      - "8001:8001"
    restart: unless-stopped
