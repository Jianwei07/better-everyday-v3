version: "3.8"

services:
  fastapi-backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: eva-fastapi
    env_file:
      - .env.local
    volumes:
      - ./chroma_storage:/api/chroma_storage
      - ./data:/api/data
    ports:
      - "8000:8000"
    depends_on:
      - llm-server
    restart: unless-stopped

  llm-server:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: eva-llm-server
    environment:
      - MODEL_ID=microsoft/phi-2
      - PORT=8001
    ports:
      - "8001:8001"
    volumes:
      - ./models:/data/models
    restart: unless-stopped
    # For llama.cpp or vLLM, replace image and env as needed

  # Optional: ChromaDB as a separate service (if not using in-process)
  # chromadb:
  #   image: chromadb/chroma:latest
  #   container_name: eva-chromadb
  #   volumes:
  #     - ./chroma_storage:/chroma_storage
  #   ports:
  #     - "8002:8000"
  #   restart: unless-stopped
