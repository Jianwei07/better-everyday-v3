services:
  fastapi-backend:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: eva-fastapi
    env_file:
      - .env.local
    volumes:
      - ./chroma_storage:./chroma_storage
      - ./data:/api/data
    ports:
      - "8000:8000"
    depends_on:
      - llm-server
    restart: unless-stopped

  llm-server:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: eva-llama-cpp
    command: ["-m", "/models/phi-2.Q4_K_M.gguf", "--port", "8001", "-c", "1024"]
    volumes:
      - ./models:/models
      - ./chroma_storage:/chroma_storage
    ports:
      - "8001:8001"
    restart: unless-stopped
